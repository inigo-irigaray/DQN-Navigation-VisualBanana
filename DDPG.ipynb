{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX==1.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d2/e08fe62f3554fbba081e80f6b23128df53b2f74ed4dcde73ec4a84dc53fb/tensorboardX-1.4-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.0MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (3.5.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorboardX==1.4) (38.4.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX==1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "20\n",
      "20\n",
      "(20, 33)\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.12805176e+00  -1.00000000e+00\n",
      "  -3.63192368e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   3.92812490e-02]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "rewards = env_info.rewards\n",
    "dones = env_info.local_done\n",
    "print(len(rewards))\n",
    "print(len(dones))\n",
    "print(states.shape)\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.029999999329447746\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 1\n",
    "\n",
    "TEST_ITERS = 1000\n",
    "\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('ExperienceFirstLast', ('state', 'action', 'reward', 'next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceSource:\n",
    "    def __init__(self, brain_name, env, agent, tracker, steps_count=2):\n",
    "        self.brain_name = brain_name\n",
    "        self.env_info = env.reset(train_mode=True)[self.brain_name]\n",
    "        self.agent = agent\n",
    "        self.tracker = tracker\n",
    "        self.steps_count = steps_count\n",
    "        self.total_rewards = np.zeros(num_agents)\n",
    "        self.next_states = None\n",
    "        self.episode = 1\n",
    "        self.episode_time = 0.0\n",
    "        self.ts = time.time()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        time_s = 0.0\n",
    "        while True:\n",
    "            if self.next_states is None:\n",
    "                states = self.env_info.vector_observations\n",
    "            else:\n",
    "                states = self.next_states\n",
    "            actions = self.agent(states)\n",
    "            self.env_info = env.step(actions)[self.brain_name]\n",
    "            rewards = self.env_info.rewards\n",
    "            self.total_rewards += rewards\n",
    "            dones = self.env_info.local_done\n",
    "            self.next_states = self.env_info.vector_observations\n",
    "            if np.any(dones):\n",
    "                exp = Experience(state=states, action=actions, reward=rewards, \n",
    "                                 next_state=self.next_states, done=dones)\n",
    "                self.episode_time = time.time() - self.ts\n",
    "                self.ts = time.time()\n",
    "                solved = self.tracker.reward(self.total_rewards, self.episode, self.episode_time)\n",
    "                if solved:\n",
    "                    break\n",
    "                self._reset()\n",
    "                yield (exp)\n",
    "            else:\n",
    "                yield(Experience(state=states, action=actions, reward=rewards, \n",
    "                                 next_state=self.next_states, done=dones))\n",
    "            \n",
    "    def _reset(self):\n",
    "        self.env_info = env.reset(train_mode=True)[self.brain_name]\n",
    "        self.total_rewards = np.zeros(num_agents)\n",
    "        self.next_states = None\n",
    "        self.episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, act_size, buffer_size, batch_size, exp_source, device=\"cpu\"):\n",
    "        assert isinstance(exp_source, ExperienceSource)\n",
    "        self.action_size = act_size\n",
    "        self.capacity = buffer_size\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.device = device\n",
    "        self.idx = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def _add(self, sample):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(sample)\n",
    "        else:\n",
    "            self.memory[self.idx] = sample\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "\n",
    "    def populate(self, samples):\n",
    "        for _ in range(samples):\n",
    "            entry = next(self.exp_source_iter)\n",
    "            state = entry.state\n",
    "            action = entry.action\n",
    "            reward = entry.reward\n",
    "            next_state = entry.next_state\n",
    "            done = entry.done\n",
    "            for i in range(num_agents):\n",
    "                exp = Experience(state=state[i,:], action=action[i,:], reward=reward[i], next_state=next_state[i,:],\n",
    "                                 done=done[i])\n",
    "                self._add(exp)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).to(self.device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_preprocessor(states):\n",
    "    np_states = np.array(states, dtype=np.float32)\n",
    "    return torch.tensor(np_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, h1=400, h2=300):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(obs_size, h1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h1, h2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h2, act_size),\n",
    "                                nn.Tanh(),\n",
    "                                )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, h1=400, h2=300):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.obs_net = nn.Sequential(nn.Linear(obs_size, h1),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        \n",
    "        self.out_net = nn.Sequential(nn.Linear(h1 + act_size, h2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(h2, 1),\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNet:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def alpha_sync(self, alpha):\n",
    "        assert isinstance(alpha, float)\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "        state = self.model.state_dict()\n",
    "        tgt_state = self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
    "        self.target_model.load_state_dict(tgt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, net, device=\"cpu\", epsilon=0.3):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, states):\n",
    "        states_v = float32_preprocessor(states).to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "        actions += self.epsilon * np.random.normal(size=actions.shape)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(self, writer, num_agents, min_ts_diff=1.0):\n",
    "        \"\"\"\n",
    "        Constructs RewardTracker\n",
    "        :param writer: writer to use for writing stats\n",
    "        :param min_ts_diff: minimal time difference to track speed\n",
    "        \"\"\"\n",
    "        self.writer = writer\n",
    "        self.min_ts_diff = min_ts_diff\n",
    "        self.mean_reward = []\n",
    "        self.rewards_window = deque(maxlen=100)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, rewards, frame, time_s):\n",
    "        self.mean_reward.append(np.mean(rewards))\n",
    "        self.rewards_window.append(np.mean(rewards))\n",
    "        print(\"Done %d episodes, mean reward %.3f, speed %.2f s/e\" % (frame, np.mean(rewards), time_s))\n",
    "        sys.stdout.flush()\n",
    "        self.writer.add_scalar(\"episode_length\", time_s, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", np.mean(self.rewards_window), frame)\n",
    "        self.writer.add_scalar(\"mean_reward\", np.mean(rewards), frame)\n",
    "        self.writer.add_scalar(\"min_reward\", np.min(rewards), frame)\n",
    "        self.writer.add_scalar(\"max_reward\", np.max(rewards), frame)\n",
    "        return self.mean_reward if np.mean(self.rewards_window) >= 30 else None\n",
    "        #if np.mean(rewards_window) >= 30:\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "Actor(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (obs_net): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out_net): Sequential(\n",
      "    (0): Linear(in_features=404, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Done 1 episodes, mean reward 0.161, speed 14.69 s/e\n",
      "Done 2 episodes, mean reward 0.841, speed 14.93 s/e\n",
      "Done 3 episodes, mean reward 1.262, speed 15.02 s/e\n",
      "Done 4 episodes, mean reward 1.089, speed 14.99 s/e\n",
      "Done 5 episodes, mean reward 0.870, speed 15.20 s/e\n",
      "Done 6 episodes, mean reward 1.080, speed 15.18 s/e\n",
      "Done 7 episodes, mean reward 1.045, speed 15.23 s/e\n",
      "Done 8 episodes, mean reward 0.971, speed 15.27 s/e\n",
      "Done 9 episodes, mean reward 0.682, speed 15.80 s/e\n",
      "Done 10 episodes, mean reward 0.757, speed 15.49 s/e\n",
      "Done 11 episodes, mean reward 0.666, speed 15.24 s/e\n",
      "Done 12 episodes, mean reward 0.768, speed 15.34 s/e\n",
      "Done 13 episodes, mean reward 0.828, speed 15.30 s/e\n",
      "Done 14 episodes, mean reward 0.961, speed 15.52 s/e\n",
      "Done 15 episodes, mean reward 0.586, speed 15.30 s/e\n",
      "Done 16 episodes, mean reward 0.844, speed 15.24 s/e\n",
      "Done 17 episodes, mean reward 0.776, speed 15.28 s/e\n",
      "Done 18 episodes, mean reward 0.911, speed 15.40 s/e\n",
      "Done 19 episodes, mean reward 1.435, speed 15.42 s/e\n",
      "Done 20 episodes, mean reward 1.471, speed 15.21 s/e\n",
      "Done 21 episodes, mean reward 1.937, speed 15.31 s/e\n",
      "Done 22 episodes, mean reward 2.617, speed 15.21 s/e\n",
      "Done 23 episodes, mean reward 2.362, speed 15.24 s/e\n",
      "Done 24 episodes, mean reward 1.847, speed 15.26 s/e\n",
      "Done 25 episodes, mean reward 1.896, speed 15.19 s/e\n",
      "Done 26 episodes, mean reward 1.978, speed 15.20 s/e\n",
      "Done 27 episodes, mean reward 2.035, speed 15.31 s/e\n",
      "Done 28 episodes, mean reward 2.034, speed 15.21 s/e\n",
      "Done 29 episodes, mean reward 2.503, speed 15.51 s/e\n",
      "Done 30 episodes, mean reward 2.450, speed 15.57 s/e\n",
      "Done 31 episodes, mean reward 2.480, speed 15.26 s/e\n",
      "Done 32 episodes, mean reward 2.881, speed 15.33 s/e\n",
      "Done 33 episodes, mean reward 3.223, speed 15.40 s/e\n",
      "Done 34 episodes, mean reward 2.677, speed 15.25 s/e\n",
      "Done 35 episodes, mean reward 3.287, speed 15.31 s/e\n",
      "Done 36 episodes, mean reward 3.277, speed 15.11 s/e\n",
      "Done 37 episodes, mean reward 4.559, speed 15.32 s/e\n",
      "Done 38 episodes, mean reward 4.497, speed 15.31 s/e\n",
      "Done 39 episodes, mean reward 4.615, speed 15.30 s/e\n",
      "Done 40 episodes, mean reward 5.847, speed 15.32 s/e\n",
      "Done 41 episodes, mean reward 6.383, speed 15.18 s/e\n",
      "Done 42 episodes, mean reward 7.758, speed 15.35 s/e\n",
      "Done 43 episodes, mean reward 9.505, speed 15.18 s/e\n",
      "Done 44 episodes, mean reward 7.926, speed 15.24 s/e\n",
      "Done 45 episodes, mean reward 9.405, speed 15.43 s/e\n",
      "Done 46 episodes, mean reward 9.367, speed 15.43 s/e\n",
      "Done 47 episodes, mean reward 12.391, speed 15.46 s/e\n",
      "Done 48 episodes, mean reward 11.202, speed 15.30 s/e\n",
      "Done 49 episodes, mean reward 12.481, speed 15.36 s/e\n",
      "Done 50 episodes, mean reward 14.939, speed 15.90 s/e\n",
      "Done 51 episodes, mean reward 15.457, speed 15.38 s/e\n",
      "Done 52 episodes, mean reward 16.660, speed 15.21 s/e\n",
      "Done 53 episodes, mean reward 15.779, speed 15.38 s/e\n",
      "Done 54 episodes, mean reward 18.396, speed 15.33 s/e\n",
      "Done 55 episodes, mean reward 20.002, speed 15.34 s/e\n",
      "Done 56 episodes, mean reward 18.789, speed 15.25 s/e\n",
      "Done 57 episodes, mean reward 18.834, speed 15.33 s/e\n",
      "Done 58 episodes, mean reward 22.420, speed 15.43 s/e\n",
      "Done 59 episodes, mean reward 24.870, speed 15.40 s/e\n",
      "Done 60 episodes, mean reward 26.915, speed 15.22 s/e\n",
      "Done 61 episodes, mean reward 27.301, speed 15.35 s/e\n",
      "Done 62 episodes, mean reward 26.236, speed 15.27 s/e\n",
      "Done 63 episodes, mean reward 28.433, speed 15.37 s/e\n",
      "Done 64 episodes, mean reward 27.419, speed 15.29 s/e\n",
      "Done 65 episodes, mean reward 30.499, speed 15.20 s/e\n",
      "Done 66 episodes, mean reward 29.477, speed 15.37 s/e\n",
      "Done 67 episodes, mean reward 30.992, speed 15.49 s/e\n",
      "Done 68 episodes, mean reward 31.458, speed 15.26 s/e\n",
      "Done 69 episodes, mean reward 32.858, speed 15.32 s/e\n",
      "Done 70 episodes, mean reward 31.290, speed 16.05 s/e\n",
      "Done 71 episodes, mean reward 34.612, speed 15.38 s/e\n",
      "Done 72 episodes, mean reward 33.225, speed 15.38 s/e\n",
      "Done 73 episodes, mean reward 34.011, speed 15.48 s/e\n",
      "Done 74 episodes, mean reward 34.095, speed 15.35 s/e\n",
      "Done 75 episodes, mean reward 32.320, speed 15.27 s/e\n",
      "Done 76 episodes, mean reward 34.727, speed 15.20 s/e\n",
      "Done 77 episodes, mean reward 34.886, speed 15.35 s/e\n",
      "Done 78 episodes, mean reward 35.630, speed 15.41 s/e\n",
      "Done 79 episodes, mean reward 36.177, speed 15.35 s/e\n",
      "Done 80 episodes, mean reward 36.958, speed 15.38 s/e\n",
      "Done 81 episodes, mean reward 36.482, speed 15.25 s/e\n",
      "Done 82 episodes, mean reward 36.716, speed 15.33 s/e\n",
      "Done 83 episodes, mean reward 35.799, speed 15.37 s/e\n",
      "Done 84 episodes, mean reward 35.712, speed 15.30 s/e\n",
      "Done 85 episodes, mean reward 35.707, speed 15.41 s/e\n",
      "Done 86 episodes, mean reward 36.027, speed 15.08 s/e\n",
      "Done 87 episodes, mean reward 35.234, speed 15.47 s/e\n",
      "Done 88 episodes, mean reward 36.628, speed 15.29 s/e\n",
      "Done 89 episodes, mean reward 36.222, speed 15.18 s/e\n",
      "Done 90 episodes, mean reward 35.810, speed 15.97 s/e\n",
      "Done 91 episodes, mean reward 36.769, speed 15.21 s/e\n",
      "Done 92 episodes, mean reward 35.419, speed 15.23 s/e\n",
      "Done 93 episodes, mean reward 35.523, speed 15.27 s/e\n",
      "Done 94 episodes, mean reward 36.512, speed 15.40 s/e\n",
      "Done 95 episodes, mean reward 35.726, speed 15.27 s/e\n",
      "Done 96 episodes, mean reward 35.984, speed 15.21 s/e\n",
      "Done 97 episodes, mean reward 35.808, speed 15.58 s/e\n",
      "Done 98 episodes, mean reward 36.716, speed 15.43 s/e\n",
      "Done 99 episodes, mean reward 36.456, speed 15.46 s/e\n",
      "Done 100 episodes, mean reward 36.664, speed 15.31 s/e\n",
      "Done 101 episodes, mean reward 37.766, speed 15.34 s/e\n",
      "Done 102 episodes, mean reward 36.930, speed 15.39 s/e\n",
      "Done 103 episodes, mean reward 36.955, speed 15.43 s/e\n",
      "Done 104 episodes, mean reward 37.868, speed 15.20 s/e\n",
      "Done 105 episodes, mean reward 37.380, speed 15.33 s/e\n",
      "Done 106 episodes, mean reward 36.345, speed 15.47 s/e\n",
      "Done 107 episodes, mean reward 33.447, speed 15.21 s/e\n",
      "Done 108 episodes, mean reward 33.537, speed 15.32 s/e\n",
      "Done 109 episodes, mean reward 35.820, speed 15.33 s/e\n",
      "Done 110 episodes, mean reward 35.164, speed 15.94 s/e\n",
      "Done 111 episodes, mean reward 34.475, speed 15.48 s/e\n",
      "Done 112 episodes, mean reward 34.040, speed 15.33 s/e\n",
      "Done 113 episodes, mean reward 36.767, speed 15.49 s/e\n",
      "Done 114 episodes, mean reward 36.232, speed 15.31 s/e\n",
      "Done 115 episodes, mean reward 35.678, speed 15.37 s/e\n",
      "Done 116 episodes, mean reward 36.807, speed 15.39 s/e\n",
      "Done 117 episodes, mean reward 37.229, speed 15.43 s/e\n",
      "Done 118 episodes, mean reward 35.977, speed 15.63 s/e\n",
      "Done 119 episodes, mean reward 36.008, speed 15.40 s/e\n",
      "Done 120 episodes, mean reward 36.328, speed 15.38 s/e\n",
      "Done 121 episodes, mean reward 35.667, speed 15.43 s/e\n",
      "Done 122 episodes, mean reward 35.468, speed 15.37 s/e\n",
      "Done 123 episodes, mean reward 35.021, speed 15.61 s/e\n",
      "Done 124 episodes, mean reward 35.464, speed 15.55 s/e\n",
      "Done 125 episodes, mean reward 36.176, speed 15.50 s/e\n",
      "Done 126 episodes, mean reward 35.971, speed 15.48 s/e\n",
      "Done 127 episodes, mean reward 38.013, speed 15.57 s/e\n",
      "Done 128 episodes, mean reward 36.712, speed 15.49 s/e\n",
      "Done 129 episodes, mean reward 36.460, speed 15.58 s/e\n",
      "Done 130 episodes, mean reward 36.175, speed 16.26 s/e\n",
      "Done 131 episodes, mean reward 36.352, speed 16.20 s/e\n",
      "Done 132 episodes, mean reward 37.572, speed 15.54 s/e\n",
      "Done 133 episodes, mean reward 36.975, speed 15.53 s/e\n",
      "Done 134 episodes, mean reward 37.109, speed 15.52 s/e\n",
      "Done 135 episodes, mean reward 37.307, speed 15.43 s/e\n",
      "Done 136 episodes, mean reward 35.803, speed 15.46 s/e\n",
      "Done 137 episodes, mean reward 36.385, speed 15.46 s/e\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-521172ff8c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mframe_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-26aebb756f6f>\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    save_path = os.path.join(\"saves\", \"d4pg-\")# \"ppo-\" + args.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')    \n",
    "    \n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "    # examine the state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "    \n",
    "    net_act = Actor(state_size, action_size).to(device)\n",
    "    net_critic = Critic(state_size, action_size).to(device)\n",
    "    print(net_act)\n",
    "    print(net_critic)\n",
    "    tgt_act = TargetNet(net_act)\n",
    "    tgt_critic = TargetNet(net_critic)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    with RewardTracker(writer, num_agents=num_agents) as tracker:\n",
    "        agent = Agent(net_act, device=device)\n",
    "        exp_source = ExperienceSource(brain_name, env, agent, tracker=tracker, steps_count=1)\n",
    "        buffer = ReplayBuffer(action_size, REPLAY_SIZE, BATCH_SIZE, exp_source, device=device)\n",
    "    \n",
    "        opt_act = optim.Adam(net_act.parameters(), lr=LEARNING_RATE)\n",
    "        opt_critic = optim.Adam(net_critic.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "\n",
    "            if len(buffer) < BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            states_v, actions_v, rewards_v, next_states_v, dones_mask = buffer.sample()\n",
    "            \"\"\"print(\"states_v {}\".format(states_v.shape))\n",
    "            print(\"actions_v {}\".format(actions_v.shape))\"\"\"\n",
    "            #print(\"rewards_v {}\".format(rewards_v.shape))\n",
    "            \"\"\"print(\"next_states_v {}\".format(next_states_v.shape))\n",
    "            print(\"dones_mask {}\".format(dones_mask.shape))\"\"\"\n",
    "\n",
    "            # train critic\n",
    "            opt_critic.zero_grad()\n",
    "            q_v = net_critic(states_v, actions_v)\n",
    "            #print(\"critic_distr {}\".format(q_v.shape))\n",
    "            next_act_v = tgt_act.target_model(next_states_v)\n",
    "            #print(tgt_critic.target_model(next_states_v, next_act_v).shape)\n",
    "            q_next_v = tgt_critic.target_model(next_states_v, next_act_v)\n",
    "            #print(\"next_distr {}\".format(q_next_v.shape))\n",
    "            q_next_v[dones_mask] = 0.0\n",
    "            #print(proj_distr_v.shape)\n",
    "            q_ref_v = rewards_v + q_next_v * GAMMA\n",
    "            #print(q_ref_v.shape)\n",
    "            critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())\n",
    "            critic_loss_v.backward()\n",
    "            opt_critic.step()\n",
    "            writer.add_scalar(\"loss_critic\", critic_loss_v, frame_idx)\n",
    "\n",
    "            # train actor\n",
    "            opt_act.zero_grad()\n",
    "            cur_actions_v = net_act(states_v)\n",
    "            actor_loss_v = -net_critic(states_v, cur_actions_v)\n",
    "            actor_loss_v = actor_loss_v.mean()\n",
    "            actor_loss_v.backward()\n",
    "            opt_act.step()\n",
    "            writer.add_scalar(\"loss_actor\", actor_loss_v, frame_idx)\n",
    "\n",
    "            tgt_act.alpha_sync(alpha=1 - 1e-3)\n",
    "            tgt_critic.alpha_sync(alpha=1 - 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
