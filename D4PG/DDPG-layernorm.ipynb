{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX==1.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d2/e08fe62f3554fbba081e80f6b23128df53b2f74ed4dcde73ec4a84dc53fb/tensorboardX-1.4-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4) (3.5.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorboardX==1.4) (38.4.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX==1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "20\n",
      "20\n",
      "(20, 33)\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.12805176e+00  -1.00000000e+00\n",
      "  -3.63192368e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   3.92812490e-02]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "rewards = env_info.rewards\n",
    "dones = env_info.local_done\n",
    "print(len(rewards))\n",
    "print(len(dones))\n",
    "print(states.shape)\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.029999999329447746\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 1\n",
    "\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 100000\n",
    "\n",
    "TEST_ITERS = 1000\n",
    "\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('ExperienceFirstLast', ('state', 'action', 'reward', 'next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceSource:\n",
    "    def __init__(self, brain_name, env, agent, tracker, steps_count=2):\n",
    "        self.brain_name = brain_name\n",
    "        self.env_info = env.reset(train_mode=True)[self.brain_name]\n",
    "        self.agent = agent\n",
    "        self.tracker = tracker\n",
    "        self.steps_count = steps_count\n",
    "        self.total_rewards = np.zeros(num_agents)\n",
    "        self.next_states = None\n",
    "        self.episode = 1\n",
    "        self.episode_time = 0.0\n",
    "        self.ts = time.time()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        time_s = 0.0\n",
    "        while True:\n",
    "            if self.next_states is None:\n",
    "                states = self.env_info.vector_observations\n",
    "            else:\n",
    "                states = self.next_states\n",
    "            actions = self.agent(states)\n",
    "            self.env_info = env.step(actions)[self.brain_name]\n",
    "            rewards = self.env_info.rewards\n",
    "            self.total_rewards += rewards\n",
    "            dones = self.env_info.local_done\n",
    "            self.next_states = self.env_info.vector_observations\n",
    "            if np.any(dones):\n",
    "                exp = Experience(state=states, action=actions, reward=rewards, \n",
    "                                 next_state=self.next_states, done=dones)\n",
    "                self.episode_time = time.time() - self.ts\n",
    "                self.ts = time.time()\n",
    "                solved = self.tracker.reward(self.total_rewards, self.episode, self.episode_time)\n",
    "                if solved:\n",
    "                    break\n",
    "                self._reset()\n",
    "                yield (exp)\n",
    "            else:\n",
    "                yield(Experience(state=states, action=actions, reward=rewards, \n",
    "                                 next_state=self.next_states, done=dones))\n",
    "            \n",
    "    def _reset(self):\n",
    "        self.env_info = env.reset(train_mode=True)[self.brain_name]\n",
    "        self.total_rewards = np.zeros(num_agents)\n",
    "        self.next_states = None\n",
    "        self.episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient `reduce`\n",
    "               operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the\n",
    "               array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must for a mathematical group together with the set of\n",
    "            possible values for array elements.\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, act_size, buffer_size, batch_size, exp_source, device=\"cpu\"):\n",
    "        assert isinstance(exp_source, ExperienceSource)\n",
    "        self.action_size = act_size\n",
    "        self.capacity = buffer_size\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.device = device\n",
    "        self.idx = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def _add(self, sample):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(sample)\n",
    "        else:\n",
    "            self.memory[self.idx] = sample\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "\n",
    "    def populate(self, samples):\n",
    "        for _ in range(samples):\n",
    "            entry = next(self.exp_source_iter)\n",
    "            state = entry.state\n",
    "            action = entry.action\n",
    "            reward = entry.reward\n",
    "            next_state = entry.next_state\n",
    "            done = entry.done\n",
    "            for i in range(num_agents):\n",
    "                exp = Experience(state=state[i,:], action=action[i,:], reward=reward[i], next_state=next_state[i,:],\n",
    "                                 done=done[i])\n",
    "                self._add(exp)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).to(self.device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, act_size, buffer_size, batch_size, exp_source, alpha, device=\"cpu\"):\n",
    "        super(PrioritizedReplayBuffer, self).__init__(act_size, buffer_size, batch_size, exp_source, device)\n",
    "        assert alpha > 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < buffer_size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def _add(self, *args, **kwargs):\n",
    "        idx = self.idx\n",
    "        super()._add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, beta):\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(self.batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "            \n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        experiences = [self.memory[idx] for idx in idxes]\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).to(self.device)\n",
    "        return states, actions, rewards, next_states, dones, idxes, weights\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_preprocessor(states):\n",
    "    np_states = np.array(states, dtype=np.float32)\n",
    "    return torch.tensor(np_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, h1=400, h2=300):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(obs_size, h1),\n",
    "                                nn.LayerNorm(h1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h1, h2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(h2, act_size),\n",
    "                                nn.Tanh(),\n",
    "                                )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size, h1=400, h2=300):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.obs_net = nn.Sequential(nn.Linear(obs_size, h1),\n",
    "                                    nn.LayerNorm(h1),\n",
    "                                    nn.ReLU(),\n",
    "                                    )\n",
    "        \n",
    "        self.out_net = nn.Sequential(nn.Linear(h1 + act_size, h2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(h2, 1),\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNet:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def alpha_sync(self, alpha):\n",
    "        assert isinstance(alpha, float)\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "        state = self.model.state_dict()\n",
    "        tgt_state = self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
    "        self.target_model.load_state_dict(tgt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, net, device=\"cpu\", epsilon=0.3):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, states):\n",
    "        states_v = float32_preprocessor(states).to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "        actions += self.epsilon * np.random.normal(size=actions.shape)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(self, writer, num_agents, min_ts_diff=1.0):\n",
    "        \"\"\"\n",
    "        Constructs RewardTracker\n",
    "        :param writer: writer to use for writing stats\n",
    "        :param min_ts_diff: minimal time difference to track speed\n",
    "        \"\"\"\n",
    "        self.writer = writer\n",
    "        self.min_ts_diff = min_ts_diff\n",
    "        self.mean_reward = []\n",
    "        self.rewards_window = deque(maxlen=100)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, rewards, frame, time_s):\n",
    "        self.mean_reward.append(np.mean(rewards))\n",
    "        self.rewards_window.append(np.mean(rewards))\n",
    "        print(\"Done %d episodes, mean reward %.3f, speed %.2f s/e\" % (frame, np.mean(rewards), time_s))\n",
    "        sys.stdout.flush()\n",
    "        self.writer.add_scalar(\"episode_length\", time_s, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", np.mean(self.rewards_window), frame)\n",
    "        self.writer.add_scalar(\"mean_reward\", np.mean(rewards), frame)\n",
    "        self.writer.add_scalar(\"min_reward\", np.min(rewards), frame)\n",
    "        self.writer.add_scalar(\"max_reward\", np.max(rewards), frame)\n",
    "        return self.mean_reward if np.mean(self.rewards_window) >= 30 else None\n",
    "        #if np.mean(rewards_window) >= 30:\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "Actor(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): LayerNorm(torch.Size([400]), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (6): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (obs_net): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=400, bias=True)\n",
      "    (1): LayerNorm(torch.Size([400]), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (out_net): Sequential(\n",
      "    (0): Linear(in_features=404, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Done 1 episodes, mean reward 0.657, speed 38.88 s/e\n",
      "Done 2 episodes, mean reward 0.894, speed 39.48 s/e\n",
      "Done 3 episodes, mean reward 0.751, speed 39.67 s/e\n",
      "Done 4 episodes, mean reward 0.353, speed 39.66 s/e\n",
      "Done 5 episodes, mean reward 0.430, speed 39.23 s/e\n",
      "Done 6 episodes, mean reward 0.239, speed 39.09 s/e\n",
      "Done 7 episodes, mean reward 0.458, speed 39.81 s/e\n",
      "Done 8 episodes, mean reward 0.606, speed 40.53 s/e\n",
      "Done 9 episodes, mean reward 0.995, speed 41.16 s/e\n",
      "Done 10 episodes, mean reward 1.269, speed 39.45 s/e\n",
      "Done 11 episodes, mean reward 1.170, speed 39.54 s/e\n",
      "Done 12 episodes, mean reward 1.889, speed 39.80 s/e\n",
      "Done 13 episodes, mean reward 2.978, speed 39.57 s/e\n",
      "Done 14 episodes, mean reward 3.015, speed 39.53 s/e\n",
      "Done 15 episodes, mean reward 3.086, speed 38.96 s/e\n",
      "Done 16 episodes, mean reward 2.787, speed 38.96 s/e\n",
      "Done 17 episodes, mean reward 2.815, speed 39.67 s/e\n",
      "Done 18 episodes, mean reward 3.218, speed 38.90 s/e\n",
      "Done 19 episodes, mean reward 3.598, speed 38.60 s/e\n",
      "Done 20 episodes, mean reward 3.178, speed 38.66 s/e\n",
      "Done 21 episodes, mean reward 5.407, speed 39.15 s/e\n",
      "Done 22 episodes, mean reward 5.271, speed 39.30 s/e\n",
      "Done 23 episodes, mean reward 6.673, speed 39.83 s/e\n",
      "Done 24 episodes, mean reward 6.106, speed 38.95 s/e\n",
      "Done 25 episodes, mean reward 6.271, speed 39.35 s/e\n",
      "Done 26 episodes, mean reward 7.860, speed 38.79 s/e\n",
      "Done 27 episodes, mean reward 10.389, speed 38.94 s/e\n",
      "Done 28 episodes, mean reward 12.155, speed 42.69 s/e\n",
      "Done 29 episodes, mean reward 10.914, speed 42.14 s/e\n",
      "Done 30 episodes, mean reward 10.304, speed 41.71 s/e\n",
      "Done 31 episodes, mean reward 14.974, speed 41.12 s/e\n",
      "Done 32 episodes, mean reward 16.077, speed 41.66 s/e\n",
      "Done 33 episodes, mean reward 15.813, speed 42.00 s/e\n",
      "Done 34 episodes, mean reward 18.269, speed 41.39 s/e\n",
      "Done 35 episodes, mean reward 19.655, speed 40.12 s/e\n",
      "Done 36 episodes, mean reward 17.673, speed 39.95 s/e\n",
      "Done 37 episodes, mean reward 18.258, speed 39.09 s/e\n",
      "Done 38 episodes, mean reward 20.648, speed 40.12 s/e\n",
      "Done 39 episodes, mean reward 19.492, speed 39.94 s/e\n",
      "Done 40 episodes, mean reward 20.578, speed 39.52 s/e\n",
      "Done 41 episodes, mean reward 18.078, speed 38.60 s/e\n",
      "Done 42 episodes, mean reward 21.555, speed 38.91 s/e\n",
      "Done 43 episodes, mean reward 23.670, speed 39.02 s/e\n",
      "Done 44 episodes, mean reward 23.579, speed 38.50 s/e\n",
      "Done 45 episodes, mean reward 24.067, speed 38.43 s/e\n",
      "Done 46 episodes, mean reward 25.280, speed 38.68 s/e\n",
      "Done 47 episodes, mean reward 23.644, speed 38.33 s/e\n",
      "Done 48 episodes, mean reward 23.254, speed 38.53 s/e\n",
      "Done 49 episodes, mean reward 26.710, speed 38.19 s/e\n",
      "Done 50 episodes, mean reward 28.543, speed 38.09 s/e\n",
      "Done 51 episodes, mean reward 31.396, speed 38.72 s/e\n",
      "Done 52 episodes, mean reward 32.445, speed 39.44 s/e\n",
      "Done 53 episodes, mean reward 33.390, speed 38.71 s/e\n",
      "Done 54 episodes, mean reward 34.716, speed 38.43 s/e\n",
      "Done 55 episodes, mean reward 34.644, speed 41.95 s/e\n",
      "Done 56 episodes, mean reward 35.909, speed 42.00 s/e\n",
      "Done 57 episodes, mean reward 36.614, speed 41.31 s/e\n",
      "Done 58 episodes, mean reward 36.395, speed 41.64 s/e\n",
      "Done 59 episodes, mean reward 36.479, speed 42.17 s/e\n",
      "Done 60 episodes, mean reward 35.380, speed 41.60 s/e\n",
      "Done 61 episodes, mean reward 35.911, speed 39.92 s/e\n",
      "Done 62 episodes, mean reward 37.120, speed 38.81 s/e\n",
      "Done 63 episodes, mean reward 38.006, speed 39.46 s/e\n",
      "Done 64 episodes, mean reward 38.436, speed 40.43 s/e\n",
      "Done 65 episodes, mean reward 38.443, speed 38.88 s/e\n",
      "Done 66 episodes, mean reward 37.419, speed 37.89 s/e\n",
      "Done 67 episodes, mean reward 37.941, speed 38.95 s/e\n",
      "Done 68 episodes, mean reward 37.175, speed 38.34 s/e\n",
      "Done 69 episodes, mean reward 38.569, speed 38.93 s/e\n",
      "Done 70 episodes, mean reward 38.057, speed 38.29 s/e\n",
      "Done 71 episodes, mean reward 38.729, speed 38.13 s/e\n",
      "Done 72 episodes, mean reward 38.107, speed 38.13 s/e\n",
      "Done 73 episodes, mean reward 37.704, speed 38.24 s/e\n",
      "Done 74 episodes, mean reward 36.353, speed 38.46 s/e\n",
      "Done 75 episodes, mean reward 33.991, speed 38.39 s/e\n",
      "Done 76 episodes, mean reward 37.384, speed 38.29 s/e\n",
      "Done 77 episodes, mean reward 38.525, speed 38.62 s/e\n",
      "Done 78 episodes, mean reward 38.190, speed 38.09 s/e\n",
      "Done 79 episodes, mean reward 37.805, speed 38.14 s/e\n",
      "Done 80 episodes, mean reward 37.570, speed 37.96 s/e\n",
      "Done 81 episodes, mean reward 36.109, speed 40.48 s/e\n",
      "Done 82 episodes, mean reward 35.731, speed 41.91 s/e\n",
      "Done 83 episodes, mean reward 34.557, speed 41.73 s/e\n",
      "Done 84 episodes, mean reward 35.276, speed 41.76 s/e\n",
      "Done 85 episodes, mean reward 36.758, speed 41.70 s/e\n",
      "Done 86 episodes, mean reward 36.548, speed 41.67 s/e\n",
      "Done 87 episodes, mean reward 37.754, speed 40.75 s/e\n",
      "Done 88 episodes, mean reward 37.631, speed 39.63 s/e\n",
      "Done 89 episodes, mean reward 37.042, speed 38.69 s/e\n",
      "Done 90 episodes, mean reward 35.074, speed 40.03 s/e\n",
      "Done 91 episodes, mean reward 37.528, speed 40.30 s/e\n",
      "Done 92 episodes, mean reward 36.879, speed 39.25 s/e\n",
      "Done 93 episodes, mean reward 37.386, speed 38.25 s/e\n",
      "Done 94 episodes, mean reward 36.884, speed 37.31 s/e\n",
      "Done 95 episodes, mean reward 35.916, speed 37.71 s/e\n",
      "Done 96 episodes, mean reward 34.576, speed 37.62 s/e\n",
      "Done 97 episodes, mean reward 35.440, speed 37.60 s/e\n",
      "Done 98 episodes, mean reward 35.601, speed 38.10 s/e\n",
      "Done 99 episodes, mean reward 34.721, speed 38.03 s/e\n",
      "Done 100 episodes, mean reward 34.884, speed 38.32 s/e\n",
      "Done 101 episodes, mean reward 36.884, speed 37.94 s/e\n",
      "Done 102 episodes, mean reward 34.869, speed 37.98 s/e\n",
      "Done 103 episodes, mean reward 35.808, speed 38.09 s/e\n",
      "Done 104 episodes, mean reward 37.482, speed 37.83 s/e\n",
      "Done 105 episodes, mean reward 36.932, speed 37.93 s/e\n",
      "Done 106 episodes, mean reward 37.232, speed 38.04 s/e\n",
      "Done 107 episodes, mean reward 36.363, speed 37.47 s/e\n",
      "Done 108 episodes, mean reward 35.937, speed 37.87 s/e\n",
      "Done 109 episodes, mean reward 37.332, speed 38.14 s/e\n",
      "Done 110 episodes, mean reward 37.011, speed 38.18 s/e\n",
      "Done 111 episodes, mean reward 37.493, speed 40.46 s/e\n",
      "Done 112 episodes, mean reward 36.306, speed 41.00 s/e\n",
      "Done 113 episodes, mean reward 38.011, speed 41.08 s/e\n",
      "Done 114 episodes, mean reward 37.249, speed 41.20 s/e\n",
      "Done 115 episodes, mean reward 37.530, speed 40.98 s/e\n",
      "Done 116 episodes, mean reward 37.626, speed 41.29 s/e\n",
      "Done 117 episodes, mean reward 37.403, speed 41.06 s/e\n",
      "Done 118 episodes, mean reward 36.046, speed 39.80 s/e\n",
      "Done 119 episodes, mean reward 35.760, speed 38.82 s/e\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7d992b0cf4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mframe_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBETA_START\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBETA_START\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBETA_FRAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-26aebb756f6f>\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    save_path = os.path.join(\"saves\", \"d4pg-\")# \"ppo-\" + args.name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')    \n",
    "    \n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "    # examine the state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "    \n",
    "    net_act = Actor(state_size, action_size).to(device)\n",
    "    net_critic = Critic(state_size, action_size).to(device)\n",
    "    print(net_act)\n",
    "    print(net_critic)\n",
    "    tgt_act = TargetNet(net_act)\n",
    "    tgt_critic = TargetNet(net_critic)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    with RewardTracker(writer, num_agents=num_agents) as tracker:\n",
    "        agent = Agent(net_act, device=device)\n",
    "        exp_source = ExperienceSource(brain_name, env, agent, tracker=tracker, steps_count=1)\n",
    "        buffer = PrioritizedReplayBuffer(action_size, REPLAY_SIZE, BATCH_SIZE, exp_source, alpha=0.6, device=device)\n",
    "    \n",
    "        opt_act = optim.Adam(net_act.parameters(), lr=LEARNING_RATE)\n",
    "        opt_critic = optim.Adam(net_critic.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "        frame_idx = 0\n",
    "        beta = BETA_START\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            beta = min(1.0, BETA_START + frame_idx * (1.0 - BETA_START) / BETA_FRAMES)\n",
    "\n",
    "            if len(buffer) < BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            states_v, actions_v, rewards_v, next_states_v, dones_mask, indices, weights = buffer.sample(beta)\n",
    "            \"\"\"print(\"states_v {}\".format(states_v.shape))\n",
    "            print(\"actions_v {}\".format(actions_v.shape))\"\"\"\n",
    "            #print(\"rewards_v {}\".format(rewards_v.shape))\n",
    "            \"\"\"print(\"next_states_v {}\".format(next_states_v.shape))\n",
    "            print(\"dones_mask {}\".format(dones_mask.shape))\"\"\"\n",
    "\n",
    "            # train critic\n",
    "            opt_critic.zero_grad()\n",
    "            q_v = net_critic(states_v, actions_v)\n",
    "            #print(\"critic_distr {}\".format(q_v.shape))\n",
    "            next_act_v = tgt_act.target_model(next_states_v)\n",
    "            #print(tgt_critic.target_model(next_states_v, next_act_v).shape)\n",
    "            q_next_v = tgt_critic.target_model(next_states_v, next_act_v)\n",
    "            #print(\"next_distr {}\".format(q_next_v.shape))\n",
    "            q_next_v[dones_mask] = 0.0\n",
    "            #print(proj_distr_v.shape)\n",
    "            q_ref_v = rewards_v + q_next_v * GAMMA\n",
    "            #print(q_ref_v.shape)\n",
    "            critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())\n",
    "            critic_loss_v.backward()\n",
    "            opt_critic.step()\n",
    "            writer.add_scalar(\"loss_critic\", critic_loss_v, frame_idx)\n",
    "\n",
    "            # train actor\n",
    "            opt_act.zero_grad()\n",
    "            cur_actions_v = net_act(states_v)\n",
    "            actor_loss_v = -net_critic(states_v, cur_actions_v)            \n",
    "            sample_prios_v = torch.abs(actor_loss_v) + 1e-5\n",
    "            actor_loss_v = actor_loss_v.mean()\n",
    "            actor_loss_v.backward()\n",
    "            opt_act.step()\n",
    "            writer.add_scalar(\"loss_actor\", actor_loss_v, frame_idx)\n",
    "                        \n",
    "            buffer.update_priorities(indices, sample_prios_v.data.cpu().numpy())\n",
    "\n",
    "            tgt_act.alpha_sync(alpha=1 - 1e-3)\n",
    "            tgt_critic.alpha_sync(alpha=1 - 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
